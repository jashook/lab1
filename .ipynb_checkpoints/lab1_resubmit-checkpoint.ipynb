{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lab 1 - Attributes and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team: Frank Sclafni, Jan Shook, and Leticia Valadez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding\n",
    "\n",
    "## Rubric (10 pts)\n",
    "\n",
    "This initial phase focuses on understanding the project objective and requirement from a business perspective, and then converting this knowledge into a data mining problem definition, and a preliminary plan designed to achieve the objectives. A decision model, especially one built using the Decision Model and Notation standard can be used.\n",
    "\n",
    "> Describe the purpose of the data set you selected (i.e., why was this data collected in the ﬁrst place?). Describe how you would deﬁne and measure the outcomes from the dataset. That is, why is this data important and how do you know if you have mined useful knowledge from the dataset? How would you measure the effectiveness of a good prediction algorithm? Be speciﬁc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "## TV News Channel Commercial Detection\n",
    "\n",
    "Our team selected this dataset for two reasons: 1) It has a large number of instances (129,685, which is greater than the requirement of at least 30,000) and enough attributes (14, which is greater than the requirement of at least 10), and 2) It looks like an interesting dataset (detecting commercials). Initial questions of interest are how do you detect commercials from this data? Can a model be trained to detect and skip (or remove) commercials? If so, would this solution be robust enough for commercial products like TiVo?\n",
    "\n",
    "This dataset is from the UCI Machine Learning website (https://archive.ics.uci.edu/ml/datasets/TV+News+Channel+Commercial+Detection+Dataset). It consists of popular audio-visual features of video shots extracted from 150 hours of TV news broadcast of 3 Indian and 2 international news channels (30 Hours each). In the readme accompanying the data, the authors describe the potential benefits of this data as follows:\n",
    "\n",
    "> Automatic identification of commercial blocks in news videos finds a lot of applications in the domain of television broadcast analysis and monitoring. Commercials occupy almost 40-60% of total air time. Manual segmentation of commercials from thousands of TV news channels is time consuming, and economically infeasible hence prompts the need for machine learning based Method. Classifying TV News commercials is a semantic video classification problem. TV News commercials on particular news channel are combinations of video shots uniquely characterized by audio-visual presentation. Hence various audio visual features extracted from video shots are widely used for TV commercial classification. Indian News channels do not follow any particular news presentation format, have large variability and dynamic nature presenting a challenging machine learning problem. Features from 150 Hours of broadcast news videos from 5 different (3 Indian and 2 International News channels) news channels. Viz. CNNIBN, NDTV 24X7, TIMESNOW, BBC and CNN are presented in this dataset. Videos are recorded at resolution of 720 X 576 at 25 fps using a DVR and set top box. 3 Indian channels are recorded concurrently while 2 International are recorded together. Feature file preserves the order of occurrence of shots.\n",
    "\n",
    "Given this information, is the subset of Indian datasets really different from the international datasets? If so, can commercials still be identified from both Indian and international datasets the same way?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this Notebook\n",
    "\n",
    "This Jupyter (v4.3.0) notebook was developed on Windows 10 Pro (64 bit) using Anaconda v4.4.7 and Python v3.*.\n",
    "\n",
    "Packages associated with Anaconda were extracted as follows:\n",
    "\n",
    "> conda install -c anaconda pandas\n",
    "\n",
    "> conda install -c anaconda numpy \n",
    "\n",
    "In addition to the packages in Anaconda (and outside of the Anaconda ecosystem), this notebook uses Plotly (v2.2.3) for visualization. The zip file for Plotly can be found on GitHub at (https://github.com/plotly/plotly.py). You can install the Plotly packages as follows:\n",
    "\n",
    "> pip install plotly\n",
    "\n",
    "> pip install cufflinks\n",
    "\n",
    "The version of Pandas and its dependencies are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INSTALLED VERSIONS\n",
      "------------------\n",
      "commit: None\n",
      "python: 3.6.3.final.0\n",
      "python-bits: 64\n",
      "OS: Windows\n",
      "OS-release: 10\n",
      "machine: AMD64\n",
      "processor: Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\n",
      "byteorder: little\n",
      "LC_ALL: None\n",
      "LANG: None\n",
      "LOCALE: None.None\n",
      "\n",
      "pandas: 0.22.0\n",
      "pytest: 3.2.1\n",
      "pip: 9.0.1\n",
      "setuptools: 36.5.0.post20170921\n",
      "Cython: 0.26.1\n",
      "numpy: 1.13.3\n",
      "scipy: 0.19.1\n",
      "pyarrow: None\n",
      "xarray: None\n",
      "IPython: 6.1.0\n",
      "sphinx: 1.6.6\n",
      "patsy: 0.4.1\n",
      "dateutil: 2.6.1\n",
      "pytz: 2017.2\n",
      "blosc: None\n",
      "bottleneck: 1.2.1\n",
      "tables: 3.4.2\n",
      "numexpr: 2.6.2\n",
      "feather: None\n",
      "matplotlib: 2.1.0\n",
      "openpyxl: 2.4.8\n",
      "xlrd: 1.1.0\n",
      "xlwt: 1.3.0\n",
      "xlsxwriter: 1.0.2\n",
      "lxml: 4.1.0\n",
      "bs4: 4.6.0\n",
      "html5lib: 0.999999999\n",
      "sqlalchemy: 1.1.13\n",
      "pymysql: None\n",
      "psycopg2: None\n",
      "jinja2: 2.9.6\n",
      "s3fs: None\n",
      "fastparquet: None\n",
      "pandas_gbq: None\n",
      "pandas_datareader: None\n",
      "Wall time: 5.3 s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%time pd.show_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding\n",
    "\n",
    "## Rubric (80 pts)\n",
    "\n",
    "The data understanding phase starts with an initial data collection and proceeds with activities in order to get familiar with the data, to identify data quality problems, to discover first insights into the data, or to detect interesting subsets to form hypotheses for hidden information.\n",
    "\n",
    "> [10 points] Describe the meaning and type of data (scale, values, etc.) for each attribute in the data ﬁle.\n",
    "\n",
    "> [15 points] Verify data quality: Explain any missing values, duplicate data, and outliers. Are those mistakes? How do you deal with these problems? Give justiﬁcations for your methods.\n",
    "\n",
    "> [10 points] Visualize appropriate statistics (e.g., range, mode, mean, median, variance, counts) for a subset of attributes. Describe anything meaningful you found from this or if you found something potentially interesting. Note: You can also use data from other sources for comparison. Explain why the statistics run are meaningful.\n",
    "\n",
    "> [15 points] Visualize the most interesting attributes (at least 5 attributes, your opinion on what is interesting). Important: Interpret the implications for each visualization. Explain for each attribute why the chosen visualization is appropriate.  \n",
    "Page ! of ! 17 39\n",
    " \n",
    "> [15 points] Visualize relationships between attributes: Look at the attributes via scatter plots, correlation, cross-tabulation, group-wise averages, etc. as appropriate. Explain any interesting relationships.\n",
    "\n",
    "> [10 points] Identify and explain interesting relationships between features and the class you are trying to predict (i.e., relationships with variables and the target classiﬁcation).\n",
    "\n",
    "> [5 points] Are there other features that could be added to the data or created from existing features?  Which ones?\n",
    "\n",
    "## Exceptional Work Rubric (10 pts)\n",
    "\n",
    "> [10 points total] You have free reign to provide additional analyses. One idea: implement dimensionality reduction, then visualize and interpret the results.\n",
    "\n",
    "\n",
    "## About this Dataset (Summary)\n",
    "\n",
    "This project is comprised of five datasets (bbc.txt, cnn.txt, cnnibn.txt, ndtv.txt, and timesnow.txt), all found at the UCI Machine Learning webset at https://archive.ics.uci.edu/ml/datasets/TV+News+Channel+Commercial+Detection+Dataset. Combined, these five datasets have 129,685 instances (rows) and 14 attributes. As shown in the example record below, most of these attributes have multiple data points (often hundreds) and almost all of these values are floating point.\n",
    "\n",
    "> 1  1:123 2:1.316440 3:1.516003 4:5.605905 5:5.346760 6:0.013233 7:0.010729 8:0.091743 9:0.050768 10:3808.067871 11:702.992493 12:7533.133301 13:1390.499268 14:971.098511 15:1894.978027 16:114.965019 17:45.018257 18:0.635224 19:0.095226 20:0.063398 21:0.061210 22:0.038319 23:0.018285 24:0.011113 25:0.007736 26:0.004864 27:0.004220 28:0.003273 29:0.002699 30:0.002553 31:0.002323 32:0.002108 33:0.002036 34:0.001792 35:0.001553 36:0.001250 37:0.001317 38:0.001084 39:0.000818 40:0.000624 41:0.000586 42:0.000529 43:0.000426 44:0.000359 45:0.000446 46:0.000268 47:0.000221 48:0.000154 49:0.000217 50:0.000193 51:0.000163 52:0.000165 53:0.000210 54:0.000114 55:0.000130 56:0.000055 57:0.000013 58:0.733037 59:0.133122 60:0.041263 61:0.019699 62:0.010962 63:0.006927 64:0.004525 65:0.003128 66:0.002314 67:0.001762 68:0.001361 69:0.001065 70:0.000914 71:0.000777 72:0.000667 73:0.000565 74:0.000520 75:0.000467 76:0.000469 77:0.000486 78:0.000417 79:0.000427 80:0.000349 81:0.000258 82:0.000262 83:0.000344 84:0.000168 85:0.000163 86:0.001058 90:0.020584 91:0.185038 92:0.148316 93:0.047098 94:0.169797 95:0.061318 96:0.002200 97:0.010440 98:0.004463 100:0.010558 101:0.002067 102:0.338970 103:0.470364 104:0.189997 105:0.018296 106:0.126517 107:0.047620 108:0.045863 109:0.184865 110:0.095976 111:0.015295 112:0.056323 113:0.024587 115:0.037647 116:0.006015 117:0.160327 118:0.251688 119:0.176144 123:0.006356 219:0.002119 276:0.002119 296:0.341102 448:0.099576 491:0.069915 572:0.141949 573:0.103814 601:0.002119 623:0.050847 726:0.038136 762:0.036017 816:0.036017 871:0.016949 924:0.008475 959:0.036017 1002:0.006356 1016:0.008475 1048:0.002119 4124:0.422333825949 4125:0.663917631952\n",
    "\n",
    "All five datasets are formated in the svmlight / libsvm format. This format is a text-based format, with one sample per line. It is a light format meaning it does not store zero valued features, every fetature that is \"missing\" has a value of zero. The first element of each line is used to store a target variable, and in this case it is the vaue of the atriburtes below. \n",
    "\n",
    "Hence, the file simply contains more records like the one shown above. While there are only 14 attributes in each dataset, most attributes can have more than one column of data. \n",
    "\n",
    "## Description of Attributes\n",
    "\n",
    "The following sections describe this dataset using the Readme.txt file, examination of the data, and definition of the terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We are using a Pandas dataframe to tabulate the data (and provide an simple introduction into Pandas)\n",
    "\n",
    "#Dimension Index  ... first column\n",
    "\n",
    "df_attributes = pd.DataFrame(\n",
    "  data=[\n",
    "    ('Dimension Index','0','integer',''),\n",
    "    ('Shot Length','1','integer',''),\n",
    "    ('Motion Distribution','2-3','float','Mean and Variance'),\n",
    "    ('Frame Difference Distribution','4-5','float','Mean and Variance'),\n",
    "    ('Short time energy','6-7','float','Mean and Variance'),\n",
    "    ('ZCR','8-9','float','Mean and Variance'),\n",
    "    ('Spectral Centroid','10-11','float','Mean and Variance'),\n",
    "    ('Spectral Roll off','12-13','float','Mean and Variance'),\n",
    "    ('Spectral Flux','14-15','float','Mean and Variance'),\n",
    "    ('Fundamental Frequency','16-17','float','Mean and Variance'),\n",
    "    ('Motion Distribution','18-58','float','40 bins'),\n",
    "    ('Frame Difference Distribution','59-91','float','32 bins'),\n",
    "    ('Text area distribution','92-122','float','15 bins Mean and 15 bins for variance'),\n",
    "    ('Bag of Audio Words','123-4123','float','4,000 bins'), \n",
    "    ('Edge change Ratio','4124-4125','float','Mean and Variance')\n",
    "  ],\n",
    "  columns=[\n",
    "    'Attribute Name','Columns','Datatype','Notes'\n",
    "  ],\n",
    "  index=[\n",
    "    'Attribute 00', 'Attribute 01', 'Attribute 02', 'Attribute 03', 'Attribute 04', 'Attribute 05', 'Attribute 06',\n",
    "    'Attribute 07', 'Attribute 08', 'Attribute 09', 'Attribute 10', 'Attribute 11', 'Attribute 12', 'Attribute 13',\n",
    "    'Attribute 14'\n",
    "  ]\n",
    ")\n",
    "\n",
    "# we will later omit the Bag of Audio Words attribute,\"123-4123\" to reduce the sparcity of the data.\n",
    "# tabulate is used to left justify these string value columns (versus the right-justified default)\n",
    "\n",
    "#from tabulate import tabulate\n",
    "\n",
    "#print(tabulate(df_attributes, showindex=True, headers=df_attributes.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute Name    Dimension Index\n",
      "Columns                         0\n",
      "Datatype                  integer\n",
      "Notes                            \n",
      "Name: Attribute 00, dtype: object\n",
      "Attribute Name    Shot Length\n",
      "Columns                     1\n",
      "Datatype              integer\n",
      "Notes                        \n",
      "Name: Attribute 01, dtype: object\n",
      "Attribute Name    Motion Distribution\n",
      "Columns                           2-3\n",
      "Datatype                        float\n",
      "Notes               Mean and Variance\n",
      "Name: Attribute 02, dtype: object\n",
      "Attribute Name    Frame Difference Distribution\n",
      "Columns                                     4-5\n",
      "Datatype                                  float\n",
      "Notes                         Mean and Variance\n",
      "Name: Attribute 03, dtype: object\n",
      "Attribute Name    Short time energy\n",
      "Columns                         6-7\n",
      "Datatype                      float\n",
      "Notes             Mean and Variance\n",
      "Name: Attribute 04, dtype: object\n",
      "Attribute Name                  ZCR\n",
      "Columns                         8-9\n",
      "Datatype                      float\n",
      "Notes             Mean and Variance\n",
      "Name: Attribute 05, dtype: object\n",
      "Attribute Name    Spectral Centroid\n",
      "Columns                       10-11\n",
      "Datatype                      float\n",
      "Notes             Mean and Variance\n",
      "Name: Attribute 06, dtype: object\n",
      "Attribute Name    Spectral Roll off\n",
      "Columns                       12-13\n",
      "Datatype                      float\n",
      "Notes             Mean and Variance\n",
      "Name: Attribute 07, dtype: object\n",
      "Attribute Name        Spectral Flux\n",
      "Columns                       14-15\n",
      "Datatype                      float\n",
      "Notes             Mean and Variance\n",
      "Name: Attribute 08, dtype: object\n",
      "Attribute Name    Fundamental Frequency\n",
      "Columns                           16-17\n",
      "Datatype                          float\n",
      "Notes                 Mean and Variance\n",
      "Name: Attribute 09, dtype: object\n",
      "Attribute Name    Motion Distribution\n",
      "Columns                         18-58\n",
      "Datatype                        float\n",
      "Notes                         40 bins\n",
      "Name: Attribute 10, dtype: object\n",
      "Attribute Name    Frame Difference Distribution\n",
      "Columns                                   59-91\n",
      "Datatype                                  float\n",
      "Notes                                   32 bins\n",
      "Name: Attribute 11, dtype: object\n",
      "Attribute Name                   Text area distribution\n",
      "Columns                                          92-122\n",
      "Datatype                                          float\n",
      "Notes             15 bins Mean and 15 bins for variance\n",
      "Name: Attribute 12, dtype: object\n",
      "Attribute Name    Bag of Audio Words\n",
      "Columns                     123-4123\n",
      "Datatype                       float\n",
      "Notes                     4,000 bins\n",
      "Name: Attribute 13, dtype: object\n",
      "Attribute Name    Edge change Ratio\n",
      "Columns                   4124-4125\n",
      "Datatype                      float\n",
      "Notes             Mean and Variance\n",
      "Name: Attribute 14, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#print(df_temp1[0].column)\n",
    "\n",
    "#pd.set_option('display.max_row', 1000)\n",
    "#pd.set_option('display.max_columns', 150)\n",
    "\n",
    "df_attributes.rename(columns={0: 'Dimension Index'}, inplace=True)\n",
    "df_attributes.rename(columns={1: 'Shot'}, inplace=True)\n",
    "df_attributes.rename(columns={2: 'Motion Distribution-Mean'}, inplace=True)\n",
    "df_attributes.rename(columns={3: 'Motion Distribution-Variance'}, inplace=True)\n",
    "df_attributes.rename(columns={4: 'Frame Difference Distribution-Mean'}, inplace=True)\n",
    "df_attributes.rename(columns={5: 'Frame Difference Distribution-Variance'}, inplace=True)\n",
    "df_attributes.rename(columns={6: 'Short time energy-Mean'}, inplace=True)\n",
    "df_attributes.rename(columns={7: 'Short time energy-Variance'}, inplace=True)\n",
    "df_attributes.rename(columns={8: 'ZCR-Mean'}, inplace=True)\n",
    "df_attributes.rename(columns={9: 'ZCR-Variance'}, inplace=True)\n",
    "df_attributes.rename(columns={10: 'Spectral Centroid-Mean'}, inplace=True)\n",
    "df_attributes.rename(columns={11: 'Spectral Centroid-Variance'}, inplace=True)\n",
    "df_attributes.rename(columns={12: 'Spectral Roll off-Mean'}, inplace=True)\n",
    "df_attributes.rename(columns={13: 'Spectral Roll off-Variance'}, inplace=True)\n",
    "df_attributes.rename(columns={14: 'Spectral Flux-Mean'}, inplace=True)\n",
    "df_attributes.rename(columns={15: 'Spectral Flux-Variance'}, inplace=True)\n",
    "df_attributes.rename(columns={16: 'Fundamental Frequency-Mean'}, inplace=True)\n",
    "df_attributes.rename(columns={17: 'Fundamental Frequency-Variance'}, inplace=True)\n",
    "df_attributes.rename(columns={18: 'Motion Distribution-Bin 1'}, inplace=True)\n",
    "df_attributes.rename(columns={19: 'Motion Distribution-Bin 2'}, inplace=True)\n",
    "df_attributes.rename(columns={20: 'Motion Distribution-Bin 3'}, inplace=True)\n",
    "df_attributes.rename(columns={21: 'Motion Distribution-Bin 4'}, inplace=True)\n",
    "df_attributes.rename(columns={22: 'Motion Distribution-Bin 5'}, inplace=True)\n",
    "df_attributes.rename(columns={23: 'Motion Distribution-Bin 6'}, inplace=True)\n",
    "df_attributes.rename(columns={24: 'Motion Distribution-Bin 7'}, inplace=True)\n",
    "df_attributes.rename(columns={25: 'Motion Distribution-Bin 8'}, inplace=True)\n",
    "df_attributes.rename(columns={26: 'Motion Distribution-Bin 9'}, inplace=True)\n",
    "df_attributes.rename(columns={27: 'Motion Distribution-Bin 10'}, inplace=True)\n",
    "df_attributes.rename(columns={28: 'Motion Distribution-Bin 11'}, inplace=True)\n",
    "df_attributes.rename(columns={29: 'Motion Distribution-Bin 12'}, inplace=True)\n",
    "df_attributes.rename(columns={30: 'Motion Distribution-Bin 13'}, inplace=True)\n",
    "df_attributes.rename(columns={31: 'Motion Distribution-Bin 14'}, inplace=True)\n",
    "df_attributes.rename(columns={32: 'Motion Distribution-Bin 15'}, inplace=True)\n",
    "df_attributes.rename(columns={33: 'Motion Distribution-Bin 16'}, inplace=True)\n",
    "df_attributes.rename(columns={34: 'Motion Distribution-Bin 17'}, inplace=True)\n",
    "df_attributes.rename(columns={35: 'Motion Distribution-Bin 18'}, inplace=True)\n",
    "df_attributes.rename(columns={36: 'Motion Distribution-Bin 19'}, inplace=True)\n",
    "df_attributes.rename(columns={37: 'Motion Distribution-Bin 20'}, inplace=True)\n",
    "df_attributes.rename(columns={38: 'Motion Distribution-Bin 21'}, inplace=True)\n",
    "df_attributes.rename(columns={39: 'Motion Distribution-Bin 22'}, inplace=True)\n",
    "df_attributes.rename(columns={40: 'Motion Distribution-Bin 23'}, inplace=True)\n",
    "df_attributes.rename(columns={41: 'Motion Distribution-Bin 24'}, inplace=True)\n",
    "df_attributes.rename(columns={42: 'Motion Distribution-Bin 25'}, inplace=True)\n",
    "df_attributes.rename(columns={43: 'Motion Distribution-Bin 26'}, inplace=True)\n",
    "df_attributes.rename(columns={44: 'Motion Distribution-Bin 27'}, inplace=True)\n",
    "df_attributes.rename(columns={45: 'Motion Distribution-Bin 28'}, inplace=True)\n",
    "df_attributes.rename(columns={46: 'Motion Distribution-Bin 29'}, inplace=True)\n",
    "df_attributes.rename(columns={47: 'Motion Distribution-Bin 30'}, inplace=True)\n",
    "df_attributes.rename(columns={48: 'Motion Distribution-Bin 31'}, inplace=True)\n",
    "df_attributes.rename(columns={49: 'Motion Distribution-Bin 32'}, inplace=True)\n",
    "df_attributes.rename(columns={50: 'Motion Distribution-Bin 33'}, inplace=True)\n",
    "df_attributes.rename(columns={51: 'Motion Distribution-Bin 34'}, inplace=True)\n",
    "df_attributes.rename(columns={52: 'Motion Distribution-Bin 35'}, inplace=True)\n",
    "df_attributes.rename(columns={53: 'Motion Distribution-Bin 36'}, inplace=True)\n",
    "df_attributes.rename(columns={54: 'Motion Distribution-Bin 37'}, inplace=True)\n",
    "df_attributes.rename(columns={55: 'Motion Distribution-Bin 38'}, inplace=True)\n",
    "df_attributes.rename(columns={56: 'Motion Distribution-Bin 39'}, inplace=True)\n",
    "df_attributes.rename(columns={57: 'Motion Distribution-Bin 40'}, inplace=True)\n",
    "\n",
    "# NOTE: Attribute 58 should be Bin 40 ... don't know what's wrong (other than readme.txt)\n",
    "\n",
    "df_attributes.rename(columns={58: 'Attribute 58 should be Bin 40'}, inplace=True)\n",
    "\n",
    "df_attributes.rename(columns={59: 'Frame Difference Distribution-Bin 1'}, inplace=True)\n",
    "df_attributes.rename(columns={60: 'Frame Difference Distribution-Bin 2'}, inplace=True)\n",
    "df_attributes.rename(columns={61: 'Frame Difference Distribution-Bin 3'}, inplace=True)\n",
    "df_attributes.rename(columns={62: 'Frame Difference Distribution-Bin 4'}, inplace=True)\n",
    "df_attributes.rename(columns={63: 'Frame Difference Distribution-Bin 5'}, inplace=True)\n",
    "df_attributes.rename(columns={64: 'Frame Difference Distribution-Bin 6'}, inplace=True)\n",
    "df_attributes.rename(columns={65: 'Frame Difference Distribution-Bin 7'}, inplace=True)\n",
    "df_attributes.rename(columns={66: 'Frame Difference Distribution-Bin 8'}, inplace=True)\n",
    "df_attributes.rename(columns={67: 'Frame Difference Distribution-Bin 9'}, inplace=True)\n",
    "df_attributes.rename(columns={68: 'Frame Difference Distribution-Bin 10'}, inplace=True)\n",
    "df_attributes.rename(columns={69: 'Frame Difference Distribution-Bin 11'}, inplace=True)\n",
    "df_attributes.rename(columns={70: 'Frame Difference Distribution-Bin 12'}, inplace=True)\n",
    "df_attributes.rename(columns={71: 'Frame Difference Distribution-Bin 13'}, inplace=True)\n",
    "df_attributes.rename(columns={72: 'Frame Difference Distribution-Bin 14'}, inplace=True)\n",
    "df_attributes.rename(columns={73: 'Frame Difference Distribution-Bin 15'}, inplace=True)\n",
    "df_attributes.rename(columns={74: 'Frame Difference Distribution-Bin 16'}, inplace=True)\n",
    "df_attributes.rename(columns={75: 'Frame Difference Distribution-Bin 17'}, inplace=True)\n",
    "df_attributes.rename(columns={76: 'Frame Difference Distribution-Bin 18'}, inplace=True)\n",
    "df_attributes.rename(columns={77: 'Frame Difference Distribution-Bin 19'}, inplace=True)\n",
    "df_attributes.rename(columns={78: 'Frame Difference Distribution-Bin 20'}, inplace=True)\n",
    "df_attributes.rename(columns={79: 'Frame Difference Distribution-Bin 21'}, inplace=True)\n",
    "df_attributes.rename(columns={80: 'Frame Difference Distribution-Bin 22'}, inplace=True)\n",
    "df_attributes.rename(columns={81: 'Frame Difference Distribution-Bin 23'}, inplace=True)\n",
    "df_attributes.rename(columns={82: 'Frame Difference Distribution-Bin 24'}, inplace=True)\n",
    "df_attributes.rename(columns={83: 'Frame Difference Distribution-Bin 25'}, inplace=True)\n",
    "df_attributes.rename(columns={84: 'Frame Difference Distribution-Bin 26'}, inplace=True)\n",
    "df_attributes.rename(columns={85: 'Frame Difference Distribution-Bin 27'}, inplace=True)\n",
    "df_attributes.rename(columns={86: 'Frame Difference Distribution-Bin 28'}, inplace=True)\n",
    "df_attributes.rename(columns={87: 'Frame Difference Distribution-Bin 29'}, inplace=True)\n",
    "df_attributes.rename(columns={88: 'Frame Difference Distribution-Bin 30'}, inplace=True)\n",
    "df_attributes.rename(columns={89: 'Frame Difference Distribution-Bin 31'}, inplace=True)\n",
    "df_attributes.rename(columns={90: 'Frame Difference Distribution-Bin 32'}, inplace=True)\n",
    "\n",
    "# NOTE: Attribute 91 should be Bin 32 ... don't know what's wrong (other than readme.txt)\n",
    "\n",
    "df_attributes.rename(columns={91: 'Attribute 91 should be Bin 32'}, inplace=True)\n",
    "\n",
    "df_attributes.rename(columns={92: 'Text area distribution-Bin 1-Mean'}, inplace=True)\n",
    "df_attributes.rename(columns={93: 'Text area distribution-Bin 2-Mean'}, inplace=True)\n",
    "df_attributes.rename(columns={94: 'Text area distribution-Bin 3-Mean'}, inplace=True)\n",
    "df_attributes.rename(columns={95: 'Text area distribution-Bin 4-Mean'}, inplace=True)\n",
    "df_attributes.rename(columns={96: 'Text area distribution-Bin 5-Mean'}, inplace=True)\n",
    "df_attributes.rename(columns={97: 'Text area distribution-Bin 6-Mean'}, inplace=True)\n",
    "df_attributes.rename(columns={98: 'Text area distribution-Bin 7-Mean'}, inplace=True)\n",
    "df_attributes.rename(columns={99: 'Text area distribution-Bin 8-Mean'}, inplace=True)\n",
    "df_attributes.rename(columns={100: 'Text area distribution-Bin 9-Mean'}, inplace=True)\n",
    "df_attributes.rename(columns={101: 'Text area distribution-Bin 10-Mean'}, inplace=True)\n",
    "df_attributes.rename(columns={102: 'Text area distribution-Bin 11-Mean'}, inplace=True)\n",
    "df_attributes.rename(columns={103: 'Text area distribution-Bin 12-Mean'}, inplace=True)\n",
    "df_attributes.rename(columns={104: 'Text area distribution-Bin 13-Mean'}, inplace=True)\n",
    "df_attributes.rename(columns={105: 'Text area distribution-Bin 14-Mean'}, inplace=True)\n",
    "df_attributes.rename(columns={106: 'Text area distribution-Bin 15-Mean'}, inplace=True)\n",
    "df_attributes.rename(columns={107: 'Text area distribution-Bin 1-Variance'}, inplace=True)\n",
    "df_attributes.rename(columns={108: 'Text area distribution-Bin 2-Variance'}, inplace=True)\n",
    "df_attributes.rename(columns={109: 'Text area distribution-Bin 3-Variance'}, inplace=True)\n",
    "df_attributes.rename(columns={110: 'Text area distribution-Bin 4-Variance'}, inplace=True)\n",
    "df_attributes.rename(columns={111: 'Text area distribution-Bin 5-Variance'}, inplace=True)\n",
    "df_attributes.rename(columns={112: 'Text area distribution-Bin 6-Variance'}, inplace=True)\n",
    "df_attributes.rename(columns={113: 'Text area distribution-Bin 7-Variance'}, inplace=True)\n",
    "df_attributes.rename(columns={114: 'Text area distribution-Bin 8-Variance'}, inplace=True)\n",
    "df_attributes.rename(columns={115: 'Text area distribution-Bin 9-Variance'}, inplace=True)\n",
    "df_attributes.rename(columns={116: 'Text area distribution-Bin 10-Variance'}, inplace=True)\n",
    "df_attributes.rename(columns={117: 'Text area distribution-Bin 11-Variance'}, inplace=True)\n",
    "df_attributes.rename(columns={118: 'Text area distribution-Bin 12-Variance'}, inplace=True)\n",
    "df_attributes.rename(columns={119: 'Text area distribution-Bin 13-Variance'}, inplace=True)\n",
    "df_attributes.rename(columns={120: 'Text area distribution-Bin 14-Variance'}, inplace=True)\n",
    "df_attributes.rename(columns={121: 'Text area distribution-Bin 15-Variance'}, inplace=True)\n",
    "\n",
    "# NOTE: Attribute 122 should be Bin 15-Variance ... don't know what's wrong (other than readme.txt)\n",
    "\n",
    "df_attributes.rename(columns={122: 'Attribute 122 should be Bin 15-Variance'}, inplace=True)\n",
    "\n",
    "df_attributes.rename(columns={121: 'Text area distribution-Bin 15-Variance'}, inplace=True)\n",
    "\n",
    "for index, row in df_attributes.iterrows():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "This section covers the activities needed to construct the dataset that will be fed into the models. The files for this project  (bbc.txt, cnn.txt, cnnibn.txt, ndtv.txt, and timesnow.txt) can be found at  https://archive.ics.uci.edu/ml/datasets/TV+News+Channel+Commercial+Detection+Dataset as a single ZIP file. To eliminate  manual work and streamline file processing, these five files were extracted and put on a team member's website (http://www.shookfamily.org) as follows:\n",
    "\n",
    "http://www.shookfamily.org/data/BBC.txt (17,720 lines)\n",
    "\n",
    "http://www.shookfamily.org/data/CNN.txt (22,545 lines)\n",
    "\n",
    "http://www.shookfamily.org/data/CNNIBN.txt (33,117 lines)\n",
    "\n",
    "http://www.shookfamily.org/data/NDTV.txt (17,051 lines)\n",
    "\n",
    "http://www.shookfamily.org/data/TIMESNOW.txt (39,252 lines)\n",
    "\n",
    "As shown in the cells below, it takes several steps to download the files and process them into the final dataset.\n",
    "\n",
    "The overall goal is to download the files from the internet and load them into an in-memory object. Because these files are stored in the SVM Light format, they are first loaded into a scipy.sparse matrix array object. These sparse matrix arrays are then inspected to eliminate as many columns as possible, and, consequently, reduce the sparseness of the matrix. Once that is accomplished, the scipy.sparse matrix arrays are converted to Pandas DataFrames for faster data processing and input into the accompanying data models.\n",
    "\n",
    "\n",
    "## Step 1: Download Files\n",
    "\n",
    "The first step in this proces is to download the five files from the internet. The data is in a pickled (marshalled / serialized) format used to persist an SVM Light dataset. The SVM Light format is basically an Index : Value pair where the index represents an element in a sparse matrix array and the value associated with that element. For example, a partial record like the following:\n",
    "\n",
    "> 1 1:123 2:1.316440 3:1.516003 ...\n",
    "\n",
    "represents the Y-axis lable followed by the X-Axis values where the first, second, and third elements are a sparse matrix array with the values 123, 1.316440, and 1.516003 (or array[0] == 123, array[1] == 1.316440, and array[2] == 1.516003. The code below downloads each SVM Light file from the internet as a scipy.sparse matrix object and converts this to as two numpy arrays X and Y representing the X axis and the Y axis.\n",
    "\n",
    "Runtime Expectation: It takes about 30 to 60 seconds to download and convert these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading datasets from the internet ...\n",
      "\n",
      "Downloading (as scipy.sparse matrix) ... http://www.shookfamily.org/data/BBC.txt\n",
      "Wall time: 3.95 s\n",
      "Wall time: 5.85 s\n",
      "Wall time: 8.41 s\n",
      "Wall time: 4.41 s\n",
      "Wall time: 10 s\n",
      "\n",
      "All files have been downloaded\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import tempfile\n",
    "\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "\n",
    "url_bbc      = 'http://www.shookfamily.org/data/BBC.txt'\n",
    "url_cnn      = 'http://www.shookfamily.org/data/CNN.txt'\n",
    "url_cnnibn   = 'http://www.shookfamily.org/data/CNNIBN.txt'\n",
    "url_ndtv     = 'http://www.shookfamily.org/data/NDTV.txt'\n",
    "url_timesnow = 'http://www.shookfamily.org/data/TIMESNOW.txt'\n",
    "\n",
    "################################################################################\n",
    "# Download file to a temporary file. Load that file into a scipy.sparse matrix\n",
    "# array, and then return that object to the caller.\n",
    "################################################################################\n",
    "\n",
    "def get_pickled_file(url):\n",
    "    response = urllib.request.urlopen(url)\n",
    "    data = response.read()      # a `bytes` object\n",
    "    text = data.decode('utf-8') # a `str`; this step can't be used if data is binary\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(delete=False, mode='w') as file_handle:\n",
    "        assert text is not None\n",
    "        file_handle.write(text)\n",
    "        filename = file_handle.name\n",
    "\n",
    "        return load_svmlight_file(filename)   # Returns the X axis and  Y axis\n",
    "\n",
    "################################################################################\n",
    "# Dowload files as scipy.sparse matrix arrays\n",
    "################################################################################\n",
    "\n",
    "print('Downloading datasets from the internet ...\\n')\n",
    "print('Downloading (as scipy.sparse matrix) ...', url_bbc)\n",
    "\n",
    "%time X1, y1 = get_pickled_file(url_bbc)\n",
    "%time X2, y2 = get_pickled_file(url_cnn)\n",
    "%time X3, y3 = get_pickled_file(url_cnnibn)\n",
    "%time X4, y4 = get_pickled_file(url_ndtv)\n",
    "%time X5, y5 = get_pickled_file(url_timesnow)\n",
    "\n",
    "print('\\nAll files have been downloaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute Descriptions\n",
    "\n",
    "### Dimension Index\n",
    "\n",
    "This is the dependent variable of Commercial (+1) or Non-Commercial (-1) (i.e., the classification).\n",
    "\n",
    "### Shot Length \n",
    "Commercial video shots are usually short in length, fast visual transitions with peculiar placement of overlaid text bands. Video Shot Length is directly used as one of the feature.\n",
    "\n",
    "### Short time energy\n",
    "Short term energy can be used for voiced, unvoiced and silence classification of speech. The relation for finding the short term energy can be derived from the total energy relation defined in signal processing.The total energy of an energy signal is given by.\n",
    "\n",
    "### ZCR\n",
    "Zero Crossing Rate (aka ZCR) is the rate of sign-changes along a signal. This is used in both speech recognition and music information retrieval and it is a feature used to classify sounds. That is percicely its use case here in this dataset, it till be used as ont of the attributes to help differenciate commercials from the news program. \n",
    "\n",
    "### Spectral Centroid\n",
    "Spectral Centroid is a measure of the “center of gravity” using the fourier transform's frequency and magnitude information. It is commenly used in digital signal processing to help characterise a spectrum. \n",
    "\n",
    "### Spectral Roll off\n",
    "Spectral Rolloff Point is a measure measure of the amount of the right-skewedness of the power spectrum.\n",
    "\n",
    "### Spectral Flux\n",
    "Spectral flux is a measure of how quickly the power spectrum of a signal is changes. It is calculated by comparing the power spectrum for one frame against the power spectrum from the previous frame.\n",
    "\n",
    "### Fundamental Frequency\n",
    "The fundamental frequency is the lowest frequency of a wwaveform. In music, the fundamental is the musical pitch of a note that is perceived as the lowest partial present.\n",
    "\n",
    "### Motion Distribution\n",
    "Motion Distribution is obtained by first computing dense optical flow (Horn-Schunk formulation) followed by construction of a distribution of flow magnitudes over the entire shot with 40 uniformly divided bins in range of [0, 40].\n",
    "\n",
    "### Frame Difference Distribution\n",
    "The Frame Difference Distribution is the measure of the difference between the current frame and a reference frame, often called \"background image\", or \"background model\". This will assist in measuring the percieved speed at which the frames appear to differientate. Sudden changes in pixel intensities are grasped by Frame Difference Distribution. Such changes are not registered by optical flow. Thus, Frame Difference Distribution is also computed along with flow magnitude distributions. The researchers obtain the frame difference by averaging absolute frame difference in each of 3 color channels and the distribution is constructed with 32 bins in the range of [0, 255] .\n",
    "\n",
    "### Text area distribution\n",
    "The Test Difference Distribution is simular to the Test Difference Distribution in that is is the measure of the difference between the current text on screen and a reference amount of text. The text distribution feature is obtained by averaging the fraction of text area present in a grid block over all frames of the shot.\n",
    "\n",
    "### Bag of Audio Words\n",
    "This attribute is to be removed to reduce the sparsness of the data set.\n",
    "\n",
    "### Edge change Ratio\n",
    "Edge Change Ratio Captures the motion of edges between consecutive frames and is defined as ratio of displaced edge pixels to the total number of edge pixels in a frame. The researchers calculated the mean and variance of the ECR over the entire shot.\n",
    "\n",
    "Note: This cell runs in just a few seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Pivot the Y-axis\n",
    "\n",
    "The Y-axis variables (y1, y2, y3, y4, y5) are returned from the cell above as arrays with a column-wise orientation:\n",
    "\n",
    "> array([ 1.,  1.,  1., ...,  1.,  1.,  1.])\n",
    "\n",
    "The code below pivots those arrays to a row-wise orientation:\n",
    " \n",
    "> array([[ 1.],\n",
    ">        [ 1.],\n",
    ">        [ 1.],\n",
    ">        ..., \n",
    ">        [ 1.],\n",
    ">        [ 1.],\n",
    ">        [ 1.]])\n",
    "\n",
    "Runtime Expectation: It takes less than a second to run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "Wall time: 0 ns\n",
      "Wall time: 0 ns\n",
      "Wall time: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time Y1 = y1[:, None]\n",
    "%time Y2 = y2[:, None]\n",
    "%time Y3 = y3[:, None]\n",
    "%time Y4 = y4[:, None]\n",
    "%time Y5 = y5[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Convert Sparse Matrix Array to an Array\n",
    "\n",
    "The first five cells display some information about each sparse matrix array. The last cell converts those sparse matrix array into a dense array.\n",
    "\n",
    "Runtime Expectation: It takes less than a second to run the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<17720x4125 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1813150 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<22545x4125 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2895841 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<33117x4125 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4189576 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<17051x4125 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2150834 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<39252x4125 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4992517 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 76.4 ms\n",
      "Wall time: 118 ms\n",
      "Wall time: 164 ms\n",
      "Wall time: 86.8 ms\n",
      "Wall time: 205 ms\n"
     ]
    }
   ],
   "source": [
    "%time X_dense1 = X1.toarray()\n",
    "%time X_dense2 = X2.toarray()\n",
    "%time X_dense3 = X3.toarray()\n",
    "%time X_dense4 = X4.toarray()\n",
    "%time X_dense5 = X5.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Concatenate the Y-axis before the X-axis\n",
    "\n",
    "Now that the Y-axis has been pivoted from a column-wise orientation to a row-wise orientation, we can concatenate the two arrays so the Y-axis is inserted before the X-axis. This places the Dependent Variable in the first column followed by the Independent Variables.\n",
    "\n",
    "Runtime Expectation: It takes about 10 to 15 seconds to run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 426 ms\n",
      "Wall time: 530 ms\n",
      "Wall time: 798 ms\n",
      "Wall time: 434 ms\n",
      "Wall time: 1.01 s\n"
     ]
    }
   ],
   "source": [
    "%time concat1 = np.hstack((Y1, X_dense1))\n",
    "%time concat2 = np.hstack((Y2, X_dense2))\n",
    "%time concat3 = np.hstack((Y3, X_dense3))\n",
    "%time concat4 = np.hstack((Y4, X_dense4))\n",
    "%time concat5 = np.hstack((Y5, X_dense5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Convert the Arrays to Pandas Dataframes\n",
    "\n",
    "The follow code simply converst the concatenated dense arrays into Pandas dataframes (to get them into the Pandas ecosystem).\n",
    "\n",
    "Runtime Expectation: It takes just a few seconds to run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'complete_data1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'complete_data1' is not defined"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'complete_data2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'complete_data2' is not defined"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'complete_data3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'complete_data3' is not defined"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'complete_data4' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'complete_data4' is not defined"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'complete_data5' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'complete_data5' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-baf4bece9d35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df1' is not defined"
     ]
    }
   ],
   "source": [
    "%time df1 = pd.DataFrame(complete_data1)\n",
    "%time df2 = pd.DataFrame(complete_data2)\n",
    "%time df3 = pd.DataFrame(complete_data3)\n",
    "%time df4 = pd.DataFrame(complete_data4)\n",
    "%time df5 = pd.DataFrame(complete_data5)\n",
    "\n",
    "print('\\n')\n",
    "print(len(df1.index), len(df2.index), len(df3.index), len(df4.index), len(df5.index))\n",
    "print(len(df1.index) + len(df2.index) + len(df3.index) + len(df4.index) + len(df5.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Concatenate the Five Pandas Dataframes\n",
    "\n",
    "This step concatenates the five Pandas dataframes into a single dataframe.\n",
    "\n",
    "Runtime Expectation: It takes about 15 to 20 seconds to run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time df_concat = pd.concat([df1, df2, df3, df4, df5])\n",
    "\n",
    "print('\\n', len(df_concat.columns), len(df_concat.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Delete Empty Columns\n",
    "\n",
    "As a sparse matrix array converted into a dense array, there is naturally a lot of sparseness in the data. In addition 4,000 of 4,126 columns represent a Bag of Words (columns 123 - 4123). Hence, the following code deletes all columns where ALL the rows in that column are zero.\n",
    "\n",
    "Note: This cell executes in about 40 to 60 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time df_concat = df_concat.loc[:, (df_concat != 0).any(axis=0)]\n",
    "\n",
    "print('\\n', len(df_concat.columns), len(df_concat.index), '\\n\\n', df_concat.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Further Inspection of the Data ---- CURRENT WORK-IN-PROGRESS\n",
    "\n",
    "As shown is the output above, 3,894 of the 4,126 columns were completely empty (and, thus, deleted).\n",
    "\n",
    "The code below takes a deeper look at columns with some missing values (versus ALL missing values).\n",
    "\n",
    "Note: This cell executes in ... seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If the values in the dataframe were NaN, this would work great. However, they are zeroes.\n",
    "# So the functon below does not work correctly. I need to figure out a way to convert df.isnull().sum()\n",
    "# to something that equivalent to df_concat == 0). I've tried quite a few different things.\n",
    "# Any help here will be much appreciated!\n",
    "\n",
    "#mis_val = df_concat.columns((df_concat == 0)).sum() <<< not working\n",
    "\n",
    "def missing_values_table(df): \n",
    "        mis_val = df.isnull().sum()\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        return mis_val_table_ren_columns\n",
    "\n",
    "total = missing_values_table(df_concat)\n",
    "\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Plot The Date - Brokend Down by News Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X1, y1 = (\"bbc.txt\")\n",
    "Y = y1[:, None]\n",
    "X_dense = X1.toarray()\n",
    "complete_data = np.hstack((Y, X_dense))\n",
    "df_plot_bbc = pd.DataFrame(complete_data)\n",
    "\n",
    "# X2, y2 = (\"cnn.txt\")\n",
    "Y2 = y2[:, None]\n",
    "X2_dense = X2.toarray()\n",
    "complete_data2 = np.hstack((Y2, X2_dense))\n",
    "df_plot_cnn = pd.DataFrame(complete_data2)\n",
    "\n",
    "# X3, y3 = (\"cnnibn.txt\")\n",
    "Y3 = y3[:, None]\n",
    "X3_dense = X3.toarray()\n",
    "complete_data3 = np.hstack((Y3, X3_dense))\n",
    "df_plot_cnnibn = pd.DataFrame(complete_data3)\n",
    "\n",
    "# X4, y4 = (\"ndtv.txt\")\n",
    "Y4 = y4[:, None]\n",
    "X4_dense = X4.toarray()\n",
    "complete_data4 = np.hstack((Y4, X4_dense))\n",
    "df_plot_ndtv = pd.DataFrame(complete_data4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "df_plot_bbc.plot(10, 12, kind = 'hexbin', gridsize = 30, sharex = False, title = 'BBC')\n",
    "df_plot_cnn.plot(10, 12, kind = 'hexbin', gridsize = 30, sharex = False, title = 'CNN')\n",
    "df_plot_cnnibn.plot(10, 12, kind = 'hexbin', gridsize = 30, sharex = False, title = 'CNNIBN')\n",
    "df_plot_ndtv.plot(10, 12, kind = 'hexbin', gridsize = 30, sharex = False, title = 'NDTV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Frankies test."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
