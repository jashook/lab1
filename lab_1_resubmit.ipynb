{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lab 1 - Attributes and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team: Frank Sclafani, Jan Shook, and Leticia Valadez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this Notebook\n",
    "\n",
    "This Jupyter (v4.3.0) notebook was developed on Windows 10 Pro (64 bit) using Anaconda v4.4.7 and Python v3.*.\n",
    "\n",
    "Packages associated with Anaconda were extracted as follows:\n",
    "\n",
    "> conda install -c anaconda pandas\n",
    "\n",
    "> conda install -c anaconda numpy \n",
    "\n",
    "In addition to the packages in Anaconda (and outside of the Anaconda ecosystem), this notebook uses Plotly (v2.2.3) for visualization. The zip file for Plotly can be found on GitHub at (https://github.com/plotly/plotly.py). You can install the Plotly packages as follows:\n",
    "\n",
    "> pip install plotly\n",
    "\n",
    "> pip install cufflinks\n",
    "\n",
    "The version of Pandas and its dependencies are shown below.\n",
    "\n",
    "Runtime Expectation: On the first execution of this notebook, this cell may take 30 seconds or so to execute. After the first execution, this cell will run in just a few seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "## TV News Channel Commercial Detection\n",
    "\n",
    "Our team selected this dataset for two reasons: 1) It has a large number of instances (129,685, which is greater than the requirement of at least 30,000) and enough attributes (14, which is greater than the requirement of at least 10), and 2) It looks like an interesting dataset (detecting commercials). Initial questions of interest are how do you detect commercials from this data? Can a model be trained to detect and skip (or remove) commercials? If so, would this solution be robust enough for commercial products like TiVo?\n",
    "\n",
    "This dataset is from the UCI Machine Learning website (https://archive.ics.uci.edu/ml/datasets/TV+News+Channel+Commercial+Detection+Dataset). It consists of popular audio-visual features of video shots extracted from 150 hours of TV news broadcast of 3 Indian and 2 international news channels (30 Hours each). In the readme accompanying the data, the authors describe the potential benefits of this data as follows:\n",
    "\n",
    "> Automatic identification of commercial blocks in news videos finds a lot of applications in the domain of television broadcast analysis and monitoring. Commercials occupy almost 40-60% of total air time. Manual segmentation of commercials from thousands of TV news channels is time consuming, and economically infeasible hence prompts the need for machine learning based Method. Classifying TV News commercials is a semantic video classification problem. TV News commercials on particular news channel are combinations of video shots uniquely characterized by audio-visual presentation. Hence various audio visual features extracted from video shots are widely used for TV commercial classification. Indian News channels do not follow any particular news presentation format, have large variability and dynamic nature presenting a challenging machine learning problem. Features from 150 Hours of broadcast news videos from 5 different (3 Indian and 2 International News channels) news channels. Viz. CNNIBN, NDTV 24X7, TIMESNOW, BBC and CNN are presented in this dataset. Videos are recorded at resolution of 720 X 576 at 25 fps using a DVR and set top box. 3 Indian channels are recorded concurrently while 2 International are recorded together. Feature file preserves the order of occurrence of shots.\n",
    "\n",
    "### Objective: Classify Video Attributes as Commercial or Non-commercial\n",
    "\n",
    "This dataset has already been classified as commercial (+1) or non-commercial (-1) in the Dimension Index attribute. Hence, in subsequent analysis, we will be able to train and compare our data models against the target variable that has already created to determine the effectiveness of the model.\n",
    "\n",
    "### Techniques Applied in this Project\n",
    "\n",
    "#### Data Preparation\n",
    "\n",
    "> The SVM Light approach to persisting sparse matrix arrays was used loaded into a Pandas dataframe\n",
    "\n",
    "> The X and Y axis in the SVM Light approach was combined into a two-dimensional Pandas dataframe\n",
    "\n",
    "> Columns that have little merit to the intial analysis were deleted\n",
    "\n",
    "> Pandas columns with empty values (i.e., all zeroes) were deleted\n",
    "\n",
    "> Different type of row and / or columns were separated into different dataframes to analyize the data differently\n",
    "\n",
    "#### Data Visualization\n",
    "\n",
    "> The Hexagon Bin Plot was used to visualize the complete dataset, and it appears a linear coorelation exists among attributes\n",
    "\n",
    "> Individual scatter plots were created for each attribute (non-bin related)\n",
    "\n",
    "### Other features \n",
    "\n",
    "* <span style=\"color:red\">A broadcast company code and/or name (there are five broadcast companies in this dataset)</span>\n",
    "* <span style=\"color:red\">The volume of the audio (commercials tend to be louder in volume than the show).</span>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%time pd.show_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## About this Dataset (Summary)\n",
    "\n",
    "This project is comprised of five datasets (bbc.txt, cnn.txt, cnnibn.txt, ndtv.txt, and timesnow.txt), all found at the UCI Machine Learning webset at https://archive.ics.uci.edu/ml/datasets/TV+News+Channel+Commercial+Detection+Dataset. Combined, these five datasets have 129,685 instances (rows) and 14 attributes. As shown in the example record below, most of these attributes have multiple data points (often hundreds) and almost all of these values are floating point.\n",
    "\n",
    "> 1  1:123 2:1.316440 3:1.516003 4:5.605905 5:5.346760 6:0.013233 7:0.010729 8:0.091743 9:0.050768 10:3808.067871 11:702.992493 12:7533.133301 13:1390.499268 14:971.098511 15:1894.978027 16:114.965019 17:45.018257 18:0.635224 19:0.095226 20:0.063398 21:0.061210 22:0.038319 23:0.018285 24:0.011113 25:0.007736 26:0.004864 27:0.004220 28:0.003273 29:0.002699 30:0.002553 31:0.002323 32:0.002108 33:0.002036 34:0.001792 35:0.001553 36:0.001250 37:0.001317 38:0.001084 39:0.000818 40:0.000624 41:0.000586 42:0.000529 43:0.000426 44:0.000359 45:0.000446 46:0.000268 47:0.000221 48:0.000154 49:0.000217 50:0.000193 51:0.000163 52:0.000165 53:0.000210 54:0.000114 55:0.000130 56:0.000055 57:0.000013 58:0.733037 59:0.133122 60:0.041263 61:0.019699 62:0.010962 63:0.006927 64:0.004525 65:0.003128 66:0.002314 67:0.001762 68:0.001361 69:0.001065 70:0.000914 71:0.000777 72:0.000667 73:0.000565 74:0.000520 75:0.000467 76:0.000469 77:0.000486 78:0.000417 79:0.000427 80:0.000349 81:0.000258 82:0.000262 83:0.000344 84:0.000168 85:0.000163 86:0.001058 90:0.020584 91:0.185038 92:0.148316 93:0.047098 94:0.169797 95:0.061318 96:0.002200 97:0.010440 98:0.004463 100:0.010558 101:0.002067 102:0.338970 103:0.470364 104:0.189997 105:0.018296 106:0.126517 107:0.047620 108:0.045863 109:0.184865 110:0.095976 111:0.015295 112:0.056323 113:0.024587 115:0.037647 116:0.006015 117:0.160327 118:0.251688 119:0.176144 123:0.006356 219:0.002119 276:0.002119 296:0.341102 448:0.099576 491:0.069915 572:0.141949 573:0.103814 601:0.002119 623:0.050847 726:0.038136 762:0.036017 816:0.036017 871:0.016949 924:0.008475 959:0.036017 1002:0.006356 1016:0.008475 1048:0.002119 4124:0.422333825949 4125:0.663917631952\n",
    "\n",
    "All five datasets are formated in the svmlight / libsvm format. This format is a text-based format, with one sample per line. It is a light format meaning it does not store zero valued features, every fetature that is \"missing\" has a value of zero. The first element of each line is used to store a target variable, and in this case it is the vaue of the atriburtes below. \n",
    "\n",
    "Hence, the file simply contains more records like the one shown above. While there are only 14 attributes in each dataset, most attributes can have more than one column of data. \n",
    "\n",
    "## Description of Attributes\n",
    "\n",
    "The following sections describe this dataset using the Readme.txt file, examination of the data, and definition of the terms.\n",
    "\n",
    "### Attribute Descriptions\n",
    "\n",
    "### Dimension Index (Dependent Variable)\n",
    "\n",
    "This is the dependent variable of Commercial (+1) or Non-Commercial (-1) (i.e., the classification).\n",
    "\n",
    "### Shot Length\n",
    "\n",
    "Commercial video shots are usually short in length, fast visual transitions with peculiar placement of overlaid text bands. Video Shot Length is directly used as one of the feature.\n",
    "\n",
    "### Short time energy\n",
    "\n",
    "Short term energy (STE) can be used for voiced, unvoiced and silence classification of speech. The relation for finding the short term energy can be derived from the total energy relation defined in signal processing. STE is defined as sum of squares of samples in an audio frame. To attract user’s attention commercials generally have higher audio amplitude leading to higher STE.\n",
    "\n",
    "### ZCR\n",
    "Zero Crossing Rate (ZCR) is the rate of sign-changes along a signal. This is used in both speech recognition and music information retrieval and it is a feature used to classify sounds. That is precisely its use here in this dataset, it will be used as one of the attributes to help differentiate commercials from the news program. The Zero Crossing Rate measures how rapidly an audio signal changes. ZCR varies significantly for non-pure speech (High ZCR), music (Moderate ZCR) and speech (Low ZCR). Usually commercials have background music along with speech and hence the use of ZCR as a feature. Audio signals associated with commercials generally have high music content and faster rate of signal change compared to that of non-commercials.\n",
    "\n",
    "### Spectral Centroid\n",
    "\n",
    "Spectral Centroid is a measure of the “center of gravity” using the Fourier transform's frequency and magnitude information. It is commonly used in digital signal processing to help characterize a spectrum. This motivated the use of spectral features where higher Spectral Centroid signify higher frequencies (music).\n",
    "\n",
    "### Spectral Roll off\n",
    "\n",
    "Spectral Roll off Point is a measure of the amount of the right-skewedness of the power spectrum. This feature discriminates between speech, music and non-pure speech.\n",
    "\n",
    "### Spectral Flux\n",
    "\n",
    "Spectral flux is a measure of how quickly the power spectrum of a signal changes. It is calculated by comparing the power spectrum for one frame against the power spectrum from the previous frame.\n",
    "\n",
    "### Fundamental Frequency\n",
    "\n",
    "The fundamental frequency is the lowest frequency of a waveform. In music, the fundamental is the musical pitch of a note that is perceived as the lowest fundamental frequency present. This feature is also used as non-commercials (dominated by pure speech) will produce lower fundamental frequencies compared to that of commercials (dominated by music).\n",
    "\n",
    "### Motion Distribution\n",
    "\n",
    "Motion Distribution is obtained by first computing dense optical flow (Horn-Schunk formulation) followed by construction of a distribution of flow magnitudes over the entire shot with 40 uniformly divided bins in range of [0, 40]. Motion Distribution is a significant feature as many previous works have indicated that commercial shots mostly have high motion content as they try to convey maximum information in minimum possible time.\n",
    "\n",
    "### Frame Difference Distribution\n",
    "\n",
    "The Frame Difference Distribution is the measure of the difference between the current frame and a reference frame, often called \"background image\", or \"background model\". This will assist in measuring the perceived speed at which the frames appear to differentiate. Sudden changes in pixel intensities are grasped by Frame Difference Distribution. Such changes are not registered by optical flow. Thus, Frame Difference Distribution is also computed along with flow magnitude distributions. The researchers obtain the frame difference by averaging absolute frame difference in each of 3 color channels and the distribution is constructed with 32 bins in the range of [0, 255].\n",
    "\n",
    "### Text area distribution\n",
    "\n",
    "The text area distribution is like the text area distribution in that is the measure of the difference between the current text on screen and a reference amount of text. The text distribution feature is obtained by averaging the fraction of text area present in a grid block over all frames of the shot.\n",
    "Bag of Audio Words\n",
    "This attribute is to be removed to reduce the sparseness of the data set.\n",
    "\n",
    "###  Edge change Ratio\n",
    "\n",
    "Edge Change Ratio Captures the motion of edges between consecutive frames and is defined as ratio of displaced edge pixels to the total number of edge pixels in a frame. The researchers calculated the mean and variance of the ECR over the entire shot.\n",
    "\n",
    "## Columns and Data Types\n",
    "\n",
    "The table below shows the attributes and their data types in tabular format for quick review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using a Pandas dataframe to tabulate the data (and provide an simple introduction into Pandas)\n",
    "\n",
    "df_attributes = pd.DataFrame(\n",
    "  data=[\n",
    "    ('Dimension Index','0','integer',''),\n",
    "    ('Shot Length','1','integer',''),\n",
    "    ('Motion Distribution','2-3','float','Mean and Variance'),\n",
    "    ('Frame Difference Distribution','4-5','float','Mean and Variance'),\n",
    "    ('Short time energy','6-7','float','Mean and Variance'),\n",
    "    ('ZCR','8-9','float','Mean and Variance'),\n",
    "    ('Spectral Centroid','10-11','float','Mean and Variance'),\n",
    "    ('Spectral Roll off','12-13','float','Mean and Variance'),\n",
    "    ('Spectral Flux','14-15','float','Mean and Variance'),\n",
    "    ('Fundamental Frequency','16-17','float','Mean and Variance'),\n",
    "    ('Motion Distribution','18-58','float','40 bins'),\n",
    "    ('Frame Difference Distribution','59-91','float','32 bins'),\n",
    "    ('Text area distribution','92-122','float','15 bins Mean and 15 bins for variance'),\n",
    "    ('Bag of Audio Words','123-4123','float','4,000 bins'), \n",
    "    ('Edge change Ratio','4124-4125','float','Mean and Variance')\n",
    "  ],\n",
    "  columns=[\n",
    "    'Attribute Name','Columns','Data Types','Notes'\n",
    "  ],\n",
    "  index=[\n",
    "    'Attribute 00', 'Attribute 01', 'Attribute 02', 'Attribute 03', 'Attribute 04', 'Attribute 05', 'Attribute 06',\n",
    "    'Attribute 07', 'Attribute 08', 'Attribute 09', 'Attribute 10', 'Attribute 11', 'Attribute 12', 'Attribute 13',\n",
    "    'Attribute 14'\n",
    "  ]\n",
    ")\n",
    "\n",
    "# we will later omit the Bag of Audio Words attribute,\"123-4123\" to reduce the sparcity of the data.\n",
    "# tabulate is used to left justify these string value columns (versus the right-justified default)\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "print(tabulate(df_attributes, showindex=True, headers=df_attributes.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "This section covers the activities needed to construct the dataset that will be fed into the models. The files for this project  (bbc.txt, cnn.txt, cnnibn.txt, ndtv.txt, and timesnow.txt) can be found at  https://archive.ics.uci.edu/ml/datasets/TV+News+Channel+Commercial+Detection+Dataset as a single ZIP file. To eliminate  manual work and streamline file processing, these five files were extracted and put on a team member's website (http://www.shookfamily.org) as follows:\n",
    "\n",
    "http://www.shookfamily.org/data/BBC.txt (17,720 lines)\n",
    "\n",
    "http://www.shookfamily.org/data/CNN.txt (22,545 lines)\n",
    "\n",
    "http://www.shookfamily.org/data/CNNIBN.txt (33,117 lines)\n",
    "\n",
    "http://www.shookfamily.org/data/NDTV.txt (17,051 lines)\n",
    "\n",
    "http://www.shookfamily.org/data/TIMESNOW.txt (39,252 lines)\n",
    "\n",
    "As shown in the cells below, it takes several steps to download the files and process them into the final dataset.\n",
    "\n",
    "The overall goal is to download the files from the internet and load them into an in-memory object. Because these files are stored in the SVM Light format, they are first loaded into a scipy.sparse matrix array object. These sparse matrix arrays are then inspected to eliminate as many columns as possible, and, consequently, reduce the sparseness of the matrix. Once that is accomplished, the scipy.sparse matrix arrays are converted to Pandas DataFrames for faster data processing and input into the accompanying data models.\n",
    "\n",
    "\n",
    "## Step 1: Download Files\n",
    "\n",
    "The first step in this proces is to download the five files from the internet. The data is in a pickled (marshalled / serialized) format used to persist an SVM Light dataset. The SVM Light format is basically an Index : Value pair where the index represents an element in a sparse matrix array and the value associated with that element. For example, a partial record like the following:\n",
    "\n",
    "> 1 1:123 2:1.316440 3:1.516003 ...\n",
    "\n",
    "represents the Y-axis lable followed by the X-Axis values where the first, second, and third elements are a sparse matrix array with the values 123, 1.316440, and 1.516003 (or array[0] == 123, array[1] == 1.316440, and array[2] == 1.516003. The code below downloads each SVM Light file from the internet as a scipy.sparse matrix object and converts this to as two numpy arrays X and Y representing the X axis and the Y axis.\n",
    "\n",
    "Runtime Expectation: It takes about 30 to 60 seconds to download and convert these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import urllib.request\n",
    "import tempfile\n",
    "\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "\n",
    "url_bbc      = 'http://www.shookfamily.org/data/BBC.txt'\n",
    "url_cnn      = 'http://www.shookfamily.org/data/CNN.txt'\n",
    "url_cnnibn   = 'http://www.shookfamily.org/data/CNNIBN.txt'\n",
    "url_ndtv     = 'http://www.shookfamily.org/data/NDTV.txt'\n",
    "url_timesnow = 'http://www.shookfamily.org/data/TIMESNOW.txt'\n",
    "\n",
    "################################################################################\n",
    "# Download file to a temporary file. Load that file into a scipy.sparse matrix\n",
    "# array, and then return that object to the caller.\n",
    "################################################################################\n",
    "\n",
    "def get_pickled_file(url):\n",
    "    response = urllib.request.urlopen(url)\n",
    "    data = response.read()      # a `bytes` object\n",
    "    text = data.decode('utf-8') # a `str`; this step can't be used if data is binary\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(delete=False, mode='w') as file_handle:\n",
    "        assert text is not None\n",
    "        file_handle.write(text)\n",
    "        filename = file_handle.name\n",
    "\n",
    "        return load_svmlight_file(filename)   # Returns the X axis and  Y axis\n",
    "\n",
    "################################################################################\n",
    "# Dowload files as scipy.sparse matrix arrays\n",
    "################################################################################\n",
    "\n",
    "print('Downloading datasets from the internet ...\\n')\n",
    "print('Downloading (as scipy.sparse matrix) ...', url_bbc)\n",
    "\n",
    "%time X1, y1 = get_pickled_file(url_bbc)\n",
    "%time X2, y2 = get_pickled_file(url_cnn)\n",
    "%time X3, y3 = get_pickled_file(url_cnnibn)\n",
    "%time X4, y4 = get_pickled_file(url_ndtv)\n",
    "%time X5, y5 = get_pickled_file(url_timesnow)\n",
    "\n",
    "print('\\nAll files have been downloaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Pivot the Y-axis\n",
    "\n",
    "The Y-axis variables (y1, y2, y3, y4, y5) are returned from the cell above as arrays in a column-wise orientation:\n",
    "\n",
    "> array([ 1.,  1.,  1., ...,  1.,  1.,  1.])\n",
    "\n",
    "The code below pivots those arrays to a row-wise orientation:\n",
    "\n",
    "> array(  \n",
    "&nbsp;&nbsp;[  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[ 1.],  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[ 1.],  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[ 1.],  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[ 1.],  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[ 1.],  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[ 1.]  \n",
    "&nbsp;&nbsp;]  \n",
    ")\n",
    "\n",
    "Runtime Expectation: It takes less than a second to run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "Y1 = y1[:, None]   # bbc\n",
    "Y2 = y2[:, None]   # cnn\n",
    "Y3 = y3[:, None]   # cnnibn\n",
    "Y4 = y4[:, None]   # ndtv\n",
    "Y5 = y5[:, None]   # timesnow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Convert Sparse Matrix Array to an Array\n",
    "\n",
    "The first five cells display some information about each sparse matrix array. The last cell converts those sparse matrix array into a dense array.\n",
    "\n",
    "Runtime Expectation: It takes less than a second to run the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time X1  # bbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time X2  # cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time X3  # cnnibn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time X4  # ndtv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time X5  # timesnow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "X_dense1 = X1.toarray()  # bbc\n",
    "X_dense2 = X2.toarray()  # cnn\n",
    "X_dense3 = X3.toarray()  # cnnibn\n",
    "X_dense4 = X4.toarray()  # ndtv\n",
    "X_dense5 = X5.toarray()  # timesnow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 4: Concatenate the Y-axis before the X-axis\n",
    "\n",
    "Now that the Y-axis has been pivoted from a column-wise orientation to a row-wise orientation, we can concatenate the two arrays so the Y-axis is i\n",
    "nserted before the X-axis. This places the Dependent Variable in the first column followed by the Independent Variables.\n",
    "\n",
    "Runtime Expectation: It takes about 10 to 15 seconds to run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "concat1 = np.hstack((Y1, X_dense1))  # bbc\n",
    "concat2 = np.hstack((Y2, X_dense2))  # cnn\n",
    "concat3 = np.hstack((Y3, X_dense3))  # cnnibn\n",
    "concat4 = np.hstack((Y4, X_dense4))  # ndtv\n",
    "concat5 = np.hstack((Y5, X_dense5))  # timesnow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Convert the Arrays to Pandas Dataframes\n",
    "\n",
    "The follow code simply converst the concatenated dense arrays into Pandas dataframes (to get them into the Pandas ecosystem).\n",
    "\n",
    "Runtime Expectation: It takes just a few seconds to run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_bbc      = pd.DataFrame(concat1)\n",
    "df_cnn      = pd.DataFrame(concat2)\n",
    "df_cnnibn   = pd.DataFrame(concat3)\n",
    "df_ndtv     = pd.DataFrame(concat4)\n",
    "df_timesnow = pd.DataFrame(concat5)\n",
    "\n",
    "print(len(df_bbc.index), len(df_cnn.index), len(df_cnnibn.index), len(df_ndtv.index), len(df_timesnow.index),\n",
    "    len(df_bbc.index) + len(df_cnn.index) + len(df_cnnibn.index) + len(df_ndtv.index) + len(df_timesnow.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Delete the Bag of Words\n",
    "\n",
    "4,000 of the 4,126 columns are a Bag of Words. For Lab 1, we are deleting these columns as a brute force technique to reduce the dimensions of the dataset. (We will add these dimensions back in subsequent lab assignments.)\n",
    "\n",
    "Note: The Bag of Words is deleted for each dataset so that plotting can be performed against the reduced dimensions. \n",
    "\n",
    "Runtime Expectation: This cell executes in just a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_bbc      = df_bbc.drop(np.arange(123, 4124), 1)\n",
    "df_cnn      = df_cnn.drop(np.arange(123, 4124), 1)\n",
    "df_cnnibn   = df_cnnibn.drop(np.arange(123, 4124), 1)\n",
    "df_ndtv     = df_ndtv.drop(np.arange(123, 4124), 1)\n",
    "df_timesnow = df_timesnow.drop(np.arange(123, 4124), 1)\n",
    "\n",
    "df_bbc.info()\n",
    "df_cnn.info()\n",
    "df_cnnibn.info()\n",
    "df_ndtv.info()\n",
    "df_timesnow.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As show in the output above, there are now 125 entries out of the 4,126.\n",
    "\n",
    "## Step 7: Delete Empty Columns\n",
    "\n",
    "As a sparse matrix array converted into a dense array, there is naturally a lot of sparseness in the data. In addition 4,000 of 4,126 columns represent a Bag of Words (columns 123 - 4123). \n",
    "\n",
    "<span style=\"color:red\">Originally we treated 0's as missing values for all features. This is not good for the features that are obtained from binning. For example,  for ('Frame Difference Distribution','59-91','float','32 bins'), when a bin contains 0, it does not mean the data is missing, but it means that the said bin is empty, ie no values fall into it. Missing values mean that data is not collect due to some reason. Here we have data, which is 0. Therefore we removed the code that deletes all columns where ALL the rows in that column are zero.</span>\n",
    "\n",
    "<span style=\"color:red\">After we loaded the data into pandas data frame, the column names are set to descriptive values, such as 'Shot_Length', 'Motion_Distribution_mean', 'Motion_Distribution_variance', etc.</span>\n",
    "\n",
    "Runtime Expectation: This cell executes in few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#Removed deletion of 0 values\n",
    "\n",
    "#df_bbc      = df_bbc.loc[:, (df_bbc != 0).any(axis=0)]\n",
    "#df_cnn      = df_cnn.loc[:, (df_cnn != 0).any(axis=0)]\n",
    "#df_cnnibn   = df_cnnibn.loc[:, (df_cnnibn != 0).any(axis=0)]\n",
    "#df_ndtv     = df_ndtv.loc[:, (df_ndtv != 0).any(axis=0)]\n",
    "#df_timesnow = df_timesnow.loc[:, (df_timesnow != 0).any(axis=0)]\n",
    "\n",
    "df_names = [df_bbc, df_cnn, df_cnnibn, df_ndtv, df_timesnow]\n",
    "\n",
    "for name in df_names:\n",
    "    name.rename(columns={0: 'Dimension Index'}, inplace=True)\n",
    "    name.rename(columns={1: 'Shot'}, inplace=True)\n",
    "    name.rename(columns={2: 'Motion Distribution-Mean'}, inplace=True)\n",
    "    name.rename(columns={3: 'Motion Distribution-Variance'}, inplace=True)\n",
    "    name.rename(columns={4: 'Frame Difference Distribution-Mean'}, inplace=True)\n",
    "    name.rename(columns={5: 'Frame Difference Distribution-Variance'}, inplace=True)\n",
    "    name.rename(columns={6: 'Short time energy-Mean'}, inplace=True)\n",
    "    name.rename(columns={7: 'Short time energy-Variance'}, inplace=True)\n",
    "    name.rename(columns={8: 'ZCR-Mean'}, inplace=True)\n",
    "    name.rename(columns={9: 'ZCR-Variance'}, inplace=True)\n",
    "    name.rename(columns={10: 'Spectral Centroid-Mean'}, inplace=True)\n",
    "    name.rename(columns={11: 'Spectral Centroid-Variance'}, inplace=True)\n",
    "    name.rename(columns={12: 'Spectral Roll off-Mean'}, inplace=True)\n",
    "    name.rename(columns={13: 'Spectral Roll off-Variance'}, inplace=True)\n",
    "    name.rename(columns={14: 'Spectral Flux-Mean'}, inplace=True)\n",
    "    name.rename(columns={15: 'Spectral Flux-Variance'}, inplace=True)\n",
    "    name.rename(columns={16: 'Fundamental Frequency-Mean'}, inplace=True)\n",
    "    name.rename(columns={17: 'Fundamental Frequency-Variance'}, inplace=True)\n",
    "    name.rename(columns={18: 'Motion Distribution-Bin 1'}, inplace=True)\n",
    "    name.rename(columns={19: 'Motion Distribution-Bin 2'}, inplace=True)\n",
    "    name.rename(columns={20: 'Motion Distribution-Bin 3'}, inplace=True)\n",
    "    name.rename(columns={21: 'Motion Distribution-Bin 4'}, inplace=True)\n",
    "    name.rename(columns={22: 'Motion Distribution-Bin 5'}, inplace=True)\n",
    "    name.rename(columns={23: 'Motion Distribution-Bin 6'}, inplace=True)\n",
    "    name.rename(columns={24: 'Motion Distribution-Bin 7'}, inplace=True)\n",
    "    name.rename(columns={25: 'Motion Distribution-Bin 8'}, inplace=True)\n",
    "    name.rename(columns={26: 'Motion Distribution-Bin 9'}, inplace=True)\n",
    "    name.rename(columns={27: 'Motion Distribution-Bin 10'}, inplace=True)\n",
    "    name.rename(columns={28: 'Motion Distribution-Bin 11'}, inplace=True)\n",
    "    name.rename(columns={29: 'Motion Distribution-Bin 12'}, inplace=True)\n",
    "    name.rename(columns={30: 'Motion Distribution-Bin 13'}, inplace=True)\n",
    "    name.rename(columns={31: 'Motion Distribution-Bin 14'}, inplace=True)\n",
    "    name.rename(columns={32: 'Motion Distribution-Bin 15'}, inplace=True)\n",
    "    name.rename(columns={33: 'Motion Distribution-Bin 16'}, inplace=True)\n",
    "    name.rename(columns={34: 'Motion Distribution-Bin 17'}, inplace=True)\n",
    "    name.rename(columns={35: 'Motion Distribution-Bin 18'}, inplace=True)\n",
    "    name.rename(columns={36: 'Motion Distribution-Bin 19'}, inplace=True)\n",
    "    name.rename(columns={37: 'Motion Distribution-Bin 20'}, inplace=True)\n",
    "    name.rename(columns={38: 'Motion Distribution-Bin 21'}, inplace=True)\n",
    "    name.rename(columns={39: 'Motion Distribution-Bin 22'}, inplace=True)\n",
    "    name.rename(columns={40: 'Motion Distribution-Bin 23'}, inplace=True)\n",
    "    name.rename(columns={41: 'Motion Distribution-Bin 24'}, inplace=True)\n",
    "    name.rename(columns={42: 'Motion Distribution-Bin 25'}, inplace=True)\n",
    "    name.rename(columns={43: 'Motion Distribution-Bin 26'}, inplace=True)\n",
    "    name.rename(columns={44: 'Motion Distribution-Bin 27'}, inplace=True)\n",
    "    name.rename(columns={45: 'Motion Distribution-Bin 28'}, inplace=True)\n",
    "    name.rename(columns={46: 'Motion Distribution-Bin 29'}, inplace=True)\n",
    "    name.rename(columns={47: 'Motion Distribution-Bin 30'}, inplace=True)\n",
    "    name.rename(columns={48: 'Motion Distribution-Bin 31'}, inplace=True)\n",
    "    name.rename(columns={49: 'Motion Distribution-Bin 32'}, inplace=True)\n",
    "    name.rename(columns={50: 'Motion Distribution-Bin 33'}, inplace=True)\n",
    "    name.rename(columns={51: 'Motion Distribution-Bin 34'}, inplace=True)\n",
    "    name.rename(columns={52: 'Motion Distribution-Bin 35'}, inplace=True)\n",
    "    name.rename(columns={53: 'Motion Distribution-Bin 36'}, inplace=True)\n",
    "    name.rename(columns={54: 'Motion Distribution-Bin 37'}, inplace=True)\n",
    "    name.rename(columns={55: 'Motion Distribution-Bin 38'}, inplace=True)\n",
    "    name.rename(columns={56: 'Motion Distribution-Bin 39'}, inplace=True)\n",
    "    name.rename(columns={57: 'Motion Distribution-Bin 40'}, inplace=True)\n",
    "    name.rename(columns={58: 'Attribute 58 should be Bin 40'}, inplace=True)\n",
    "    name.rename(columns={59: 'Frame Difference Distribution-Bin 1'}, inplace=True)\n",
    "    name.rename(columns={60: 'Frame Difference Distribution-Bin 2'}, inplace=True)\n",
    "    name.rename(columns={61: 'Frame Difference Distribution-Bin 3'}, inplace=True)\n",
    "    name.rename(columns={62: 'Frame Difference Distribution-Bin 4'}, inplace=True)\n",
    "    name.rename(columns={63: 'Frame Difference Distribution-Bin 5'}, inplace=True)\n",
    "    name.rename(columns={64: 'Frame Difference Distribution-Bin 6'}, inplace=True)\n",
    "    name.rename(columns={65: 'Frame Difference Distribution-Bin 7'}, inplace=True)\n",
    "    name.rename(columns={66: 'Frame Difference Distribution-Bin 8'}, inplace=True)\n",
    "    name.rename(columns={67: 'Frame Difference Distribution-Bin 9'}, inplace=True)\n",
    "    name.rename(columns={68: 'Frame Difference Distribution-Bin 10'}, inplace=True)\n",
    "    name.rename(columns={69: 'Frame Difference Distribution-Bin 11'}, inplace=True)\n",
    "    name.rename(columns={70: 'Frame Difference Distribution-Bin 12'}, inplace=True)\n",
    "    name.rename(columns={71: 'Frame Difference Distribution-Bin 13'}, inplace=True)\n",
    "    name.rename(columns={72: 'Frame Difference Distribution-Bin 14'}, inplace=True)\n",
    "    name.rename(columns={73: 'Frame Difference Distribution-Bin 15'}, inplace=True)\n",
    "    name.rename(columns={74: 'Frame Difference Distribution-Bin 16'}, inplace=True)\n",
    "    name.rename(columns={75: 'Frame Difference Distribution-Bin 17'}, inplace=True)\n",
    "    name.rename(columns={76: 'Frame Difference Distribution-Bin 18'}, inplace=True)\n",
    "    name.rename(columns={77: 'Frame Difference Distribution-Bin 19'}, inplace=True)\n",
    "    name.rename(columns={78: 'Frame Difference Distribution-Bin 20'}, inplace=True)\n",
    "    name.rename(columns={79: 'Frame Difference Distribution-Bin 21'}, inplace=True)\n",
    "    name.rename(columns={80: 'Frame Difference Distribution-Bin 22'}, inplace=True)\n",
    "    name.rename(columns={81: 'Frame Difference Distribution-Bin 23'}, inplace=True)\n",
    "    name.rename(columns={82: 'Frame Difference Distribution-Bin 24'}, inplace=True)\n",
    "    name.rename(columns={83: 'Frame Difference Distribution-Bin 25'}, inplace=True)\n",
    "    name.rename(columns={84: 'Frame Difference Distribution-Bin 26'}, inplace=True)\n",
    "    name.rename(columns={85: 'Frame Difference Distribution-Bin 27'}, inplace=True)\n",
    "    name.rename(columns={86: 'Frame Difference Distribution-Bin 28'}, inplace=True)\n",
    "    name.rename(columns={87: 'Frame Difference Distribution-Bin 29'}, inplace=True)\n",
    "    name.rename(columns={88: 'Frame Difference Distribution-Bin 30'}, inplace=True)\n",
    "    name.rename(columns={89: 'Frame Difference Distribution-Bin 31'}, inplace=True)\n",
    "    name.rename(columns={90: 'Frame Difference Distribution-Bin 32'}, inplace=True)\n",
    "    name.rename(columns={91: 'Attribute 91 should be Bin 32'}, inplace=True)\n",
    "    name.rename(columns={92: 'Text area distribution-Bin 1-Mean'}, inplace=True)\n",
    "    name.rename(columns={93: 'Text area distribution-Bin 2-Mean'}, inplace=True)\n",
    "    name.rename(columns={94: 'Text area distribution-Bin 3-Mean'}, inplace=True)\n",
    "    name.rename(columns={95: 'Text area distribution-Bin 4-Mean'}, inplace=True)\n",
    "    name.rename(columns={96: 'Text area distribution-Bin 5-Mean'}, inplace=True)\n",
    "    name.rename(columns={97: 'Text area distribution-Bin 6-Mean'}, inplace=True)\n",
    "    name.rename(columns={98: 'Text area distribution-Bin 7-Mean'}, inplace=True)\n",
    "    name.rename(columns={99: 'Text area distribution-Bin 8-Mean'}, inplace=True)\n",
    "    name.rename(columns={100: 'Text area distribution-Bin 9-Mean'}, inplace=True)\n",
    "    name.rename(columns={101: 'Text area distribution-Bin 10-Mean'}, inplace=True)\n",
    "    name.rename(columns={102: 'Text area distribution-Bin 11-Mean'}, inplace=True)\n",
    "    name.rename(columns={103: 'Text area distribution-Bin 12-Mean'}, inplace=True)\n",
    "    name.rename(columns={104: 'Text area distribution-Bin 13-Mean'}, inplace=True)\n",
    "    name.rename(columns={105: 'Text area distribution-Bin 14-Mean'}, inplace=True)\n",
    "    name.rename(columns={106: 'Text area distribution-Bin 15-Mean'}, inplace=True)\n",
    "    name.rename(columns={107: 'Text area distribution-Bin 1-Variance'}, inplace=True)\n",
    "    name.rename(columns={108: 'Text area distribution-Bin 2-Variance'}, inplace=True)\n",
    "    name.rename(columns={109: 'Text area distribution-Bin 3-Variance'}, inplace=True)\n",
    "    name.rename(columns={110: 'Text area distribution-Bin 4-Variance'}, inplace=True)\n",
    "    name.rename(columns={111: 'Text area distribution-Bin 5-Variance'}, inplace=True)\n",
    "    name.rename(columns={112: 'Text area distribution-Bin 6-Variance'}, inplace=True)\n",
    "    name.rename(columns={113: 'Text area distribution-Bin 7-Variance'}, inplace=True)\n",
    "    name.rename(columns={114: 'Text area distribution-Bin 8-Variance'}, inplace=True)\n",
    "    name.rename(columns={115: 'Text area distribution-Bin 9-Variance'}, inplace=True)\n",
    "    name.rename(columns={116: 'Text area distribution-Bin 10-Variance'}, inplace=True)\n",
    "    name.rename(columns={117: 'Text area distribution-Bin 11-Variance'}, inplace=True)\n",
    "    name.rename(columns={118: 'Text area distribution-Bin 12-Variance'}, inplace=True)\n",
    "    name.rename(columns={119: 'Text area distribution-Bin 13-Variance'}, inplace=True)\n",
    "    name.rename(columns={120: 'Text area distribution-Bin 14-Variance'}, inplace=True)\n",
    "    name.rename(columns={121: 'Text area distribution-Bin 15-Variance'}, inplace=True)\n",
    "    name.rename(columns={122: 'Attribute 122 should be Bin 15-Variance'}, inplace=True)\n",
    "    name.rename(columns={121: 'Text area distribution-Bin 15-Variance'}, inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As show in the output above, there are now 120 entries out of the 4,126.\n",
    "\n",
    "## Step 8: Inspecting Missing Values\n",
    "\n",
    "As shown is the output above, 120 columns are left in the dataframe. 4,005 columns were deleted after eliminating the Bag of Words (4,000 columns) and the five columns (88, 89, 120, 121, 123) with all zero values.\n",
    "\n",
    "### Step 8a: Display Table of Missing Values\n",
    "\n",
    "The code below displays columns with SOME missing values (versus ALL missing values).\n",
    "\n",
    "Runtime Expectation: This cell executes in about 1 or 2 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_of_zeros_table(df):\n",
    "    numberOf_nonzeros = df.astype(bool).sum(axis=0)\n",
    "    NumberOf_Zeros = df.count()-numberOf_nonzeros\n",
    "    percentOf_Zeros=NumberOf_Zeros / df.count() * 100\n",
    "    table1 = pd.concat([NumberOf_Zeros, percentOf_Zeros], axis=1)\n",
    "    table2 = table1.rename(columns={0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "    return table2\n",
    "\n",
    "df_missing_values_table1 = percentage_of_zeros_table(df_bbc)\n",
    "df_missing_values_table2 = percentage_of_zeros_table(df_cnn)\n",
    "df_missing_values_table3 = percentage_of_zeros_table(df_cnnibn)\n",
    "df_missing_values_table4 = percentage_of_zeros_table(df_ndtv)\n",
    "df_missing_values_table5 = percentage_of_zeros_table(df_timesnow)\n",
    "\n",
    "df_missing_values_table1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8b: View Missing Values via a Threshold (40%)\n",
    "\n",
    "The code below displays columns having over 40% of its values as zero.\n",
    "\n",
    "Runtime Expectation: This cell executes in about 1 or 2 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_missing_values_table1 = df_missing_values_table1[(df_missing_values_table1['% of Total Values'] > 40)]\n",
    "\n",
    "df_missing_values_table1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8c: Drop Columns with a High Ratio of Missing Values\n",
    "\n",
    "The code below drops column 87, which has about 90% of its values as zero.\n",
    "\n",
    "Runtime Expectation: This cell executes in about 1 or 2 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Drop column 87 in each of the individual datasets\n",
    "\n",
    "df_bbc      = df_bbc.drop(['Frame Difference Distribution-Bin 29'], axis=1)\n",
    "df_cnn      = df_cnn.drop(['Frame Difference Distribution-Bin 29'], axis=1)\n",
    "df_cnnibn   = df_cnnibn.drop(['Frame Difference Distribution-Bin 29'], axis=1)\n",
    "df_ndtv     = df_ndtv.drop(['Frame Difference Distribution-Bin 29'], axis=1)\n",
    "df_timesnow = df_timesnow.drop(['Frame Difference Distribution-Bin 29'], axis=1)\n",
    "\n",
    "df_bbc.info()\n",
    "df_cnn.info()\n",
    "df_cnnibn.info()\n",
    "df_ndtv.info()\n",
    "df_timesnow.info()\n",
    "\n",
    "# The code below should delete 1 columns (87)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As show in the output above, there are now 119 entries out of the 4,126.\n",
    "\n",
    "## Step 9: Concatenate the Five Pandas Dataframes\n",
    "\n",
    "This step concatenates the five Pandas dataframes into a single dataframe.\n",
    "\n",
    "Runtime Expectation: It takes about 15 to 20 seconds to run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_concat = pd.concat([df_bbc, df_cnn, df_cnnibn, df_ndtv, df_timesnow])\n",
    "\n",
    "df_concat.name = 'TV News Channel Commercial Detection'\n",
    "\n",
    "df_concat.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As show in the output above, there are now 119 entries out of the 4,126 and 129,685 rows.\n",
    "\n",
    "## Step 10: Seperate Commercial from Non-Commercial\n",
    "\n",
    "At this point, the concatenated dataframe has both commercial shots (+1) and non-commercial shots (-1) in the Dimension Index (the Dependent Variable). The following code separate these two classes of data into two dataframes.\n",
    "\n",
    "Runtime Expectation: It takes about ? to run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the first column, the Dimension Index, since the two dataframes are either commercial (+1) or non-commercial (-1)\n",
    "\n",
    "df_commercial = df_concat.loc[df_concat[\"Dimension Index\"] == 1]\n",
    "df_commercial = df_commercial.drop([\"Dimension Index\"], axis=1)  # Drop the first column, the Dimension Index, \n",
    "\n",
    "df_commercial.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Box Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Add more plots!!!!!\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#df1 = df_non_commercial[['Shot']]\n",
    "#df1.boxplot() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_commercial = df_concat.loc[df_concat['Dimension Index'] == -1]\n",
    "df_non_commercial = df_non_commercial.drop(['Dimension Index'], axis=1)  # Drop the first column, the Dimension Index, \n",
    "\n",
    "#df_non_commercial = df_non_commercial.reindex(labels=None, index=None, columns=None, axis=None, method=None, copy=True, level=None, fill_value=nan, limit=None, tolerance=None)\n",
    "df_non_commercial = df_non_commercial.reset_index(drop=True) #reindex rows\n",
    "\n",
    "df_non_commercial.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Seperate Dataframe into No Bins vs. Bins\n",
    "\n",
    "### Separate Commercial (No Bins vs. Bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = list(df_commercial)\n",
    "\n",
    "bins1 = cols[17:57]\n",
    "bins2 = cols[58:89]\n",
    "bins3 = cols[90:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_commercial_no_bins = df_commercial.drop(bins1)\n",
    "df_commercial_no_bins = df_commercial_no_bins.drop(bins1)   # Delete more columns from this dataset\n",
    "\n",
    "df_commercial_no_bins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_commercial_bins = df_commercial.drop(bins2)\n",
    "df_commercial_bins = df_commercial_bins.drop(bins2)   # Delete more columns from this dataset\n",
    "\n",
    "df_commercial_bins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_commercial_bins = df_commercial.drop(bins3)\n",
    "df_commercial_bins = df_commercial_bins.drop(bins3)   # Delete more columns from this dataset\n",
    "\n",
    "df_commercial_bins.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Summary Statistics\n",
    "\n",
    "### Step 12a: Summary of the Concatenated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12b: Summary of the Concatenated Dataset (Commercial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_commercial.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12c: Summary of the Concatenated Dataset (Non-commercial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_commercial.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12d: Observations about these Statistics\n",
    "\n",
    "The ratio between commercial and noncommercial data is about 64% to 36%.\n",
    "\n",
    "### <span style=\"color:red\"> Explain why the statistics run are meaningful: </span>\n",
    "> <span style=\"color:red\"> These statistics are meaningful because it demonstrates that the mean Shot Length is what was expected. Shot length for commercial was only 63 while commercial was 180. This demonstrates what we know is that commercials generally have a much shorter shot length and that can be a string indicator to use during classification. </span>\n",
    "\n",
    "> <span style=\"color:red\"> Also, it is interesting that the ZCR-mean between the commercial and non-commercial sets was so similar. As a reminder, the Zero Crossing Rate (ZCR) is the rate of sign-changes along a signal. This is used in both speech recognition and music information retrieval and it is a feature used to classify sounds. This infers that the speech and music in commercials is very similar to the non-commercials show in this dataset. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Data \n",
    "\n",
    "## Step 13: Plots \n",
    "\n",
    "## Step 13a: Plots for the Attributes\n",
    "\n",
    "The code below creates a plot for each of the non-binned attributes (columns 0 - 18). \n",
    "\n",
    "Runtime Expectation: This code takes 10 to 15 minutes to plot all 18 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "#for i in range(0,19):\n",
    "#    sns.pairplot(df_concat[[cols[i], cols[i+1], cols[i+2], cols[i+4], cols[i+6], cols[i+8], cols[i+10]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13b: Hexbin Plots for the Attributes\n",
    "\n",
    "The Hex bin plots below compare the relationship between the different news sources. The charts visualize  the linear relationship that all of the news networks have with the means. They will also helps identify outliers.\n",
    "\n",
    "Runtime Expectation: This code runs in just a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "fig, axs = plt.subplots(2,3)\n",
    "\n",
    "fig.set_figwidth(20)\n",
    "fig.set_figheight(12)\n",
    "\n",
    "# Plot all five datasets / broadcast\n",
    "\n",
    "df_concat.plot('Spectral Centroid-Mean','Spectral Roll off-Mean',kind='hexbin',gridsize=30,title='All Five Networks',ax=axs[0,0])\n",
    "\n",
    "# Plot each dataset / broadcast\n",
    "\n",
    "df_bbc.plot('Spectral Centroid-Mean','Spectral Roll off-Mean',kind='hexbin',gridsize=30,title='BBC',ax=axs[0,1])\n",
    "df_cnn.plot('Spectral Centroid-Mean','Spectral Roll off-Mean',kind='hexbin',gridsize=30,title='CNN',ax=axs[0,2])\n",
    "df_cnnibn.plot('Spectral Centroid-Mean','Spectral Roll off-Mean',kind='hexbin',gridsize=30,title='CNNIBN',ax=axs[1,0])\n",
    "df_ndtv.plot('Spectral Centroid-Mean','Spectral Roll off-Mean',kind='hexbin',gridsize=30,title='NDTV',ax=axs[1,1])\n",
    "df_timesnow.plot('Spectral Centroid-Mean','Spectral Roll off-Mean',kind='hexbin',gridsize=30,title='TIMESNOW',ax=axs[1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The plots below compare multible different attributes in the Commercial and Non-Commercial datasets. This shows a true distinction between the two classes in the and will help demenstrate if it is possible to distinguish between commercial and non-commercial with the data at hand.\n",
    "\n",
    "Runtime Expectation: This code runs in just a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "fig, axs = plt.subplots(1,2)\n",
    "\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "df_commercial.plot('Shot','Motion Distribution-Mean', kind='hexbin', gridsize=30,\n",
    "    title='Attribute: Commercial Shot Length', ax=axs[0])\n",
    "df_non_commercial.plot('Shot','Motion Distribution-Mean', kind='hexbin', gridsize=30,\n",
    "    title='Attribute: Non-Commercial Shot Length', ax=axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Shot Length of the Commercial and Non-Commercial seams to be close in time. This is consistant with modern tv shows and film making where typical shot lengths last for only a few seconds.\n",
    "\n",
    "Runtime Expectation: This code runs in just a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "fig, axs = plt.subplots(1,2)\n",
    "\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "df_non_commercial.plot('Motion Distribution-Bin 1', 'Attribute 58 should be Bin 40', kind='hexbin', gridsize=30,\n",
    "    title='Attribute: Non-Commercial Motion Distribution', ax=axs[0])\n",
    "df_commercial.plot('Motion Distribution-Bin 1', 'Attribute 58 should be Bin 40', kind='hexbin', gridsize=30,\n",
    "    title='Attribute: Commercial Motion Distribution', ax=axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the hexbin plots below the non-commercial and commercial difference distribution are simular with the non-commerical having a distinct grouping at zero. Further analysis is needed to discover the meaning of this feature in the data which be outliers.\n",
    "\n",
    "Runtime Expectation: This code runs in just a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "fig, axs = plt.subplots(1,2)\n",
    "\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "df_non_commercial.plot('Frame Difference Distribution-Bin 1', 'Attribute 91 should be Bin 32', kind='hexbin', gridsize=30,\n",
    "    title='Attribute: Non-Commercial Frame Difference Distribution', ax=axs[0])\n",
    "df_commercial.plot('Frame Difference Distribution-Bin 1', 'Attribute 91 should be Bin 32', kind='hexbin', gridsize=30,\n",
    "    title = 'Attribute: Commercial Frame Difference Distribution', ax=axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The comercial and non-commercial  ZCR (Zero Crossing Rate), the rate of sign-changes along a signal with the non-commerical having a distinct grouing at zero. Further analysis is needed to discover the meanign of this feature in the data.\n",
    "\n",
    "Runtime Expectation: This code runs in just a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "fig, axs = plt.subplots(1,2)\n",
    "\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "df_non_commercial.plot('ZCR-Mean', 'ZCR-Variance', kind='hexbin', gridsize=30,\n",
    "    title = 'Attribute: Non-Commercial ZCR', ax=axs[0])\n",
    "df_commercial.plot('ZCR-Mean', 'ZCR-Variance', kind='hexbin', gridsize=30,\n",
    "    title = 'Attribute: Commercial ZCR', ax=axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The hexbin plots from the Commercial and Non-Commercial plots below  demenstrate a simular positive linear relationship with the non-commerical having the more distinct linear relationship.\n",
    "\n",
    "Runtime Expectation: This code runs in just a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "fig, axs = plt.subplots(1,2)\n",
    "\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "df_non_commercial.plot('Spectral Flux-Mean', 'Spectral Flux-Variance', kind='hexbin', gridsize=30,\n",
    "    title = 'Attribute: Non-Commercial Spectral Flux', ax=axs[0])\n",
    "df_commercial.plot('Spectral Flux-Mean', 'Spectral Flux-Variance', kind='hexbin', gridsize=30,\n",
    "    title = 'Attribute: Commercial Spectral Flux', ax=axs[1])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
